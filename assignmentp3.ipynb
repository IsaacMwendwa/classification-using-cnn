{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoder_Exercises.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPQM73S_UQLk",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoders\n",
        "Autoencoders are a class of neural network that attempt to recreate the input\n",
        "as their target using back-propagation. An autoencoder consists of two parts; an **encoder** and a **decoder**. The encoder will read the input and compress it to a compact representation, and the decoder will read the compact representation and recreate the input from it. In other words, the autoencoder tries to learn the identity function by minimizing the reconstruction error. They have an inherent capability to learn\n",
        "a compact representation of data. They are at the center of deep belief networks\n",
        "and find applications in image reconstruction, clustering, machine translation,\n",
        "and much more.\n",
        "\n",
        "This exercise aims to test your understanding of autoencoder architecture, and how it can be used to denoise an image. We will build a convolutional autoencoder. Combining your knowledge of a Vanilla/Denoising Autoencoder and Convolutional Networks.\n",
        "\n",
        "The notebook has five Exercises followed by an optional exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwqKr35LhRnl",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Import Modules \n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "\n",
        "np.random.seed(11)\n",
        "tf.random.set_seed(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIgJlvDIM7Zw",
        "colab_type": "text"
      },
      "source": [
        "## AutoEncoder  Architecture\n",
        "The number of hidden units in the autoencoder is typically less than the number of input (and output) units. This forces the encoder to learn a compressed representation of the input, which the decoder reconstructs. If there is a structure in the input data in the form of correlations between input features, then the autoencoder will discover some of these correlations, and end up learning a low-dimensional representation of the data similar to that learned using principal component analysis (PCA).\n",
        "\n",
        "Once trained\n",
        "* We can discard **decoder** and use **Encoder** to optain a compact representation of input.\n",
        "* We can cascade Encoder to a classifier.\n",
        "\n",
        "The encoder and decoder components of an autoencoder can be implemented using either dense, convolutional, or recurrent networks, depending on the kind of data that is being modeled.\n",
        "\n",
        "Below we define an encoder and a decoder using Convolutional layers. Both consist of three convolutional layers. Each layer in Encoder has a corresponding layer in decoder, thus in this case it is like three autoencoders stacked over each other. This is also called **Stacked Autoencoders** \n",
        "\n",
        "![](https://drive.google.com/uc?id=1UzM67qf1VE_8akrCgiohKjUIHoO_2x4E)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71xVjHhTE5Z8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Encoder\n",
        "class Encoder(K.layers.Layer):\n",
        "    def __init__(self, filters):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = Conv2D(filters=filters[0], kernel_size=3, strides=1, activation='relu', padding='same')\n",
        "        self.conv2 = Conv2D(filters=filters[1], kernel_size=3, strides=1, activation='relu', padding='same')\n",
        "        self.conv3 = Conv2D(filters=filters[2], kernel_size=3, strides=1, activation='relu', padding='same')\n",
        "        self.pool = MaxPooling2D((2, 2), padding='same')\n",
        "               \n",
        "    \n",
        "    def call(self, input_features):\n",
        "        x = self.conv1(input_features)\n",
        "        #print(\"Ex1\", x.shape)\n",
        "        x = self.pool(x)\n",
        "        #print(\"Ex2\", x.shape)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prFVAaj_FM9L",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Decoder\n",
        "class Decoder(K.layers.Layer):\n",
        "    def __init__(self, filters):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = Conv2D(filters=filters[2], kernel_size=3, strides=1, activation='relu', padding='same')\n",
        "        self.conv2 = Conv2D(filters=filters[1], kernel_size=3, strides=1, activation='relu', padding='same')\n",
        "        self.conv3 = Conv2D(filters=filters[0], kernel_size=3, strides=1, activation='relu', padding='valid')\n",
        "        self.conv4 = Conv2D(1, 3, 1, activation='sigmoid', padding='same')\n",
        "        self.upsample = UpSampling2D((2, 2))\n",
        "  \n",
        "    def call(self, encoded):\n",
        "        x = self.conv1(encoded)\n",
        "        #print(\"dx1\", x.shape)\n",
        "        x = self.upsample(x)\n",
        "        #print(\"dx2\", x.shape)\n",
        "        x = self.conv2(x)\n",
        "        x = self.upsample(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.upsample(x)\n",
        "        return self.conv4(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xttby3Lw9pT",
        "colab_type": "text"
      },
      "source": [
        "## Denoising Autoencoder\n",
        "\n",
        "When we train the autoencoder, we can train it directly on the raw images or we can add noise to the input images while training. When the autoencoder is trained on noisy data, it gets an even interesting property--it can reconstruct noisy images. In other words--you give it an image with noise and it will remove the noise from it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y4h5e6CTvrN",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1:\n",
        "In this exercise we will train the stacked autoencoder in four steps:\n",
        "* In [Step 1](#step1) choose the noise = 0\n",
        "* Complete the [Step 2](#step2)\n",
        "* In the [Step 3](#step3) choose filters as [16, 32, 64] for Encoder and [64, 32, 16] for Decoder.\n",
        "* Perform [Step 4](#step4) for batch size of 64 and 10 epochs\n",
        "* Reflect on the plotted images what do you see?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFHypkQ2GnK",
        "colab_type": "text"
      },
      "source": [
        "**Answer 1** (Double click to edit)*italicized text*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qz9itTNs0FBG"
      },
      "source": [
        "<a id='step1'></a>\n",
        "### Step 1:\n",
        "Read the dataset, process it for noise = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGQwAOyKEi1I",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Dataset Reading and Processing\n",
        "Noise = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train / 255.\n",
        "x_test = x_test / 255.\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train),28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "noise = Noise\n",
        "x_train_noisy = x_train + noise * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0, 1)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0, 1)\n",
        "\n",
        "x_train_noisy = x_train_noisy.astype('float32')\n",
        "x_test_noisy = x_test_noisy.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P8m_M3xFfl-",
        "colab_type": "text"
      },
      "source": [
        "<a id='another_cell'></a>\n",
        "### Step 2\n",
        "\n",
        "You need to complete the code below. We will be using the Encoder and Decoder architectures that we have defined above to build an autoencoder. In the code below replace `...` with right code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfAUWna6FdNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder(K.Model):\n",
        "    def __init__(self, filters_encoder, filters_decoder):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.loss = []\n",
        "        self.encoder = Encoder(...)\n",
        "        self.decoder = Decoder(...)\n",
        "\n",
        "    def call(self, input_features):\n",
        "        #print(input_features.shape)\n",
        "        encoded = self.encoder(...)\n",
        "        #print(encoded.shape)\n",
        "        reconstructed = self.decoder(...)\n",
        "        #print(reconstructed.shape)\n",
        "        return reconstructed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4S7xGoIN2ZO_"
      },
      "source": [
        "## Exercise 2:\n",
        "In this exercise we will make only one change, in step 3 choose filters as: `[16, 32, 64]` for both Encoder and Decoder.\n",
        " Try training the Autoencoder. What happens? Why do you think it is so?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dElvF1I45wJt"
      },
      "source": [
        "**Answer 2** (Double click to edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fj2d1Qqs7_eK"
      },
      "source": [
        "## Exercise 3:\n",
        "\n",
        "Now we will introduce noise of 0.2 in the training dataset. Train an autoencoder with filters [64,32,16] for encoder and [16,32,64] for decoder and observe the reconstrucred images.\n",
        "\n",
        "\n",
        "What do you find? Is the autoencoder able to recognize noisy digits?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dNvlTwLK877U"
      },
      "source": [
        "**Answer 3** (Double click to edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BiGMz4Kd9AZk"
      },
      "source": [
        "## Exercise 4:\n",
        "\n",
        "Let us be more adventurous with the same Encoder-Decoder architecture, we increase the noise and observe the reconstrucred images.\n",
        "\n",
        "\n",
        "What do you find? Till what noise value is the autoencoder able to reconstruct images? Till what noise level you (human) can recognize the digits in the noisy image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IL5Aps7f9io1"
      },
      "source": [
        "**Answer 4** (Double click to edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVTaR4St4PvR",
        "colab_type": "text"
      },
      "source": [
        "<a id='step3'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGeWEEixZBhG",
        "colab_type": "text"
      },
      "source": [
        "### Step 3:\n",
        "\n",
        "We have built Convolutional Autoencoder. That is both Encoder and Decoder are buit using Convolutional layers. Below you need to select "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC8Bx7u0n_OP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Select Filters for Encoder & Decoder\n",
        "filter_encoder_0 = 16 #@param {type:\"slider\", min:8, max:256, step:2}\n",
        "filter_encoder_1 = 32 #@param {type:\"slider\", min:8, max:256, step:2}\n",
        "filter_encoder_2 = 64 #@param {type:\"slider\", min:8, max:256, step:2}\n",
        "\n",
        "filters_en = [filter_encoder_0,filter_encoder_1,filter_encoder_2]\n",
        "\n",
        "\n",
        "filter_decoder_0 = 16 #@param {type:\"slider\", min:8, max:256, step:2}\n",
        "filter_decoder_1 = 32 #@param {type:\"slider\", min:8, max:256, step:2}\n",
        "filter_decoder_2 = 64 #@param {type:\"slider\", min:8, max:256, step:2}\n",
        "\n",
        "filters_de = [filter_decoder_0,filter_decoder_1,filter_decoder_2]\n",
        "\n",
        "\n",
        "model = Autoencoder(filters_en, filters_de)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfpau9KBuCS3",
        "colab_type": "text"
      },
      "source": [
        "### Step 4:\n",
        "Choose the appropriate batch_size and epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7yqEyKqJ595",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "d0fe28b4-a3fe-4766-97c3-2b73c8812810"
      },
      "source": [
        "#@title Train the model\n",
        "BATCH_SIZE = 132 #@param {type:\"slider\", min:32, max:2000, step:10}\n",
        "EPOCHS = 13 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "batch_size = BATCH_SIZE\n",
        "max_epochs = EPOCHS\n",
        "loss = model.fit(x_train_noisy,\n",
        "                x_train,\n",
        "                validation_data=(x_test_noisy, x_test),\n",
        "                epochs=max_epochs,\n",
        "                batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "1875/1875 [==============================] - 136s 73ms/step - loss: 0.1829 - val_loss: 0.1531\n",
            "Epoch 2/7\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.1505 - val_loss: 0.1459\n",
            "Epoch 3/7\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.1452 - val_loss: 0.1424\n",
            "Epoch 4/7\n",
            "1875/1875 [==============================] - 134s 72ms/step - loss: 0.1423 - val_loss: 0.1409\n",
            "Epoch 5/7\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.1405 - val_loss: 0.1394\n",
            "Epoch 6/7\n",
            "1875/1875 [==============================] - 135s 72ms/step - loss: 0.1393 - val_loss: 0.1381\n",
            "Epoch 7/7\n",
            "1875/1875 [==============================] - 136s 72ms/step - loss: 0.1383 - val_loss: 0.1377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L817CFR2LgNR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Reconstructed images\n",
        "number = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for index in range(number):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, number, index + 1)\n",
        "    plt.imshow(x_test_noisy[index].reshape(28, 28), cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, number, index + 1 + number)\n",
        "    plt.imshow(tf.reshape(model(x_test_noisy)[index], (28, 28)), cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrFJlz5b94dI",
        "colab_type": "text"
      },
      "source": [
        "## Optional Exercise\n",
        "Construct a Sparse Autoencoder with Dense layer/s, train it on noisy images as before. See how the hidden dimensions influence the reconstruction. Which is one is better for denoising, the convolution Encoder/Decoder or Dense Encoder/Decoder, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz6WiM7t-Hjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}